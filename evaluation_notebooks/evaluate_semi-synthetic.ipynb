{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LiteralEvaluation - Synthetic Dataset Experiments: Tables\n",
    "\n",
    "This notebook creates the Latex tables showing the quality of model predictions on the Semi-Synthetic Dataset.\n",
    "\n",
    "This notebook uses the scores (triple likelihood) predicted by the models of the test triples from `NUM_RUNS` runs stored in `data/saved_models/saved_models_run_[run number]`. The logs produced during all our experiments (over 3 runs) are included for completeness."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T09:36:03.912862Z",
     "start_time": "2024-07-16T09:36:03.907396Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "SAVED_MODELS_PATH = '../data/saved_models'\n",
    "OUT_FILE = '../data/tex/synthetic_tab.tex'\n",
    "NUM_RUNS = 3\n",
    "\n",
    "run_name_2_model_name = json.load(open('../data/tex/constants.json'))['rank-file_2_name']\n",
    "\n",
    "# should not be modified\n",
    "dataset_name = 'Synthetic'\n",
    "value_relation = '/m/is_a'\n",
    "category_a = '/m/high'\n",
    "category_b = '/m/low'"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adjust file names\n",
    "The KGA rank files have been saved with a different naming convention. We need to adjust the file names to match the other models."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T15:06:28.276491Z",
     "start_time": "2024-06-19T15:06:28.265492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for run_i in range(1, 4):\n",
    "# Set the directory where your files are located\n",
    "    directory = os.path.join(SAVED_MODELS_PATH, f'saved_models_run_{run_i}')\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if 'distmult' is in the file name\n",
    "        new_filename = ''\n",
    "        \n",
    "        if 'QHC_5' in filename and 'Synthetic' in filename:\n",
    "            model = 'kgadistmult' if 'distmult' in filename else 'kgatucker'\n",
    "            feature_type = 'rand' if 'rand' in filename else 'numerical_literals'\n",
    "            new_filename = f'ranks_test_evaluation_{model}_Synthetic_{feature_type}_train.tsv'\n",
    "\n",
    "        if new_filename != '':\n",
    "            old_file = os.path.join(directory, filename)\n",
    "            new_file = os.path.join(directory, new_filename)\n",
    "            # Rename the file\n",
    "            os.rename(old_file, new_file)\n",
    "    \n",
    "print(\"Files have been renamed successfully.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been renamed successfully.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read in the Source Dataset (Synthetic Dataset) \n",
    "\n",
    "The source dataset contains the true triples which are required to compute the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# read the true literal values that are used to classify into the two categories (the file should only contain one literal relation type)\n",
    "df_literals = pd.read_csv(f'../data/{dataset_name}/literals/numerical_literals.txt', sep='\\t', header=None,  names=[\"head\", \"relation\", \"literal\"])\n",
    "\n",
    "# derive human entities from the class mapping file\n",
    "class_mapping_df = pd.read_csv('../data/FB15k-237_class_mapping.csv', sep=';', header=0)\n",
    "human_uris = class_mapping_df[class_mapping_df[\"class_label\"] == \"human\"][\"dataset_entity\"]\n",
    "\n",
    "df_literals[df_literals['head'].isin(human_uris.to_list())]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T09:19:15.723522Z",
     "start_time": "2024-07-16T09:19:15.696051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             head      relation   literal\n",
       "3       /m/01sl1q  /m/has_value  0.046682\n",
       "6       /m/07nznf  /m/has_value  0.375532\n",
       "8        /m/0q9kd  /m/has_value  0.130452\n",
       "15      /m/04bdxl  /m/has_value  0.718915\n",
       "17       /m/079vf  /m/has_value  0.296804\n",
       "...           ...           ...       ...\n",
       "13503   /m/09f5pp  /m/has_value  0.711507\n",
       "13647   /m/0glyyw  /m/has_value  0.024703\n",
       "13704    /m/06rkl  /m/has_value  0.616274\n",
       "13717  /m/02p59ry  /m/has_value  0.862526\n",
       "14032   /m/01hbgs  /m/has_value  0.299931\n",
       "\n",
       "[4505 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>literal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/m/01sl1q</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.046682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/m/07nznf</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.375532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/m/0q9kd</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.130452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/m/04bdxl</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.718915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/m/079vf</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.296804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13503</th>\n",
       "      <td>/m/09f5pp</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.711507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647</th>\n",
       "      <td>/m/0glyyw</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.024703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13704</th>\n",
       "      <td>/m/06rkl</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.616274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13717</th>\n",
       "      <td>/m/02p59ry</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.862526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14032</th>\n",
       "      <td>/m/01hbgs</td>\n",
       "      <td>/m/has_value</td>\n",
       "      <td>0.299931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4505 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate the models based on their predictions on the Synthetic dataset"
  },
  {
   "cell_type": "code",
   "source": [
    "# Returns the scores predicted by the model (given by df_results) for a certain tail entity.\n",
    "def get_scores_for_certain_tail(tail_entity, df_results):\n",
    "    related_scores = pd.merge(df_literals[df_literals['head'].isin(human_uris.to_list())], df_results[df_results[\"tail\"]==tail_entity][['head', 'score']], left_on='head', right_on='head')\n",
    "    related_scores = related_scores.rename(columns={ 'score': f'score {tail_entity}' })\n",
    "    return related_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T09:22:59.866786Z",
     "start_time": "2024-07-16T09:22:59.861538Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "head = \"\"\"\n",
    "\\\\begin{table}[]\n",
    "\n",
    "\\setlength{\\\\tabcolsep}{6pt}\n",
    "\\\\renewcommand{\\\\arraystretch}{1.1}\n",
    "\n",
    "\\caption{Scores achieved on the synthetic dataset. Acc$_{org}$ denotes the Acc score achieved on the synthetic dataset when we provide the meaningful synthetic literal values, whereas Acc$_{rand}$ denotes the Acc score on the synthetic dataset if we apply the random feature ablation after the dataset creation.}\n",
    "\\label{tab:synthetic}\n",
    "\\\\begin{center}\n",
    "\n",
    "\\\\begin{tabular}{l|l|l}\n",
    "\\hline\n",
    "\\\\bf{Model} & \\\\bf{Acc$_{org}$}                            &  \\\\bf{Acc$_{rand}$}                            \\\\\\\\ \\hline\n",
    "\"\"\"\n",
    "\n",
    "tail = \"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\n",
    "\\end{center}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "out = open(OUT_FILE, 'w')\n",
    "\n",
    "out.write(head)\n",
    "\n",
    "for run_name in ['ranks_test_evaluation_DistMult_0.2_0.0_literal', 'ranks_test_evaluation_ComplEx_0.2_0.0_literal', 'ranks_test_evaluation_KBLN_0.2_0.0_literal', 'ranks_test_evaluation_MTKGNN_0.2_0.0_literal', 'ranks_test_evaluation_TransEA_0.3', 'ranks_test_evaluation_kgatucker', 'ranks_test_evaluation_kgadistmult']:\n",
    "\n",
    "    line_string = f'{run_name_2_model_name[run_name]}'\n",
    "\n",
    "    for literal_type in ['numerical_literals', 'rand']:\n",
    "\n",
    "        acc_scores = []\n",
    "        mr_diff_scores = []\n",
    "\n",
    "        for i in range(1, NUM_RUNS + 1):\n",
    "\n",
    "            result_file = os.path.join(SAVED_MODELS_PATH ,f'saved_models_run_{i}/{run_name}_Synthetic_{literal_type}_train.tsv')\n",
    "\n",
    "            if not os.path.exists(result_file):\n",
    "                print(f'Run {i} does not exist')\n",
    "                print(result_file)\n",
    "            else:\n",
    "                df_results = pd.read_csv(result_file, sep='\\t', header=None,  names=[\"head\", \"relation\", \"tail\", \"rank head\", \"rank tail\", \"score\"])\n",
    "                # read the results (only value_relation triples are relevant for the evaluation, therefore we filter the dataframe)\n",
    "                df_results = df_results[df_results['relation']==value_relation]\n",
    "\n",
    "                # acc score\n",
    "                a = get_scores_for_certain_tail(category_a, df_results)\n",
    "                b = get_scores_for_certain_tail(category_b, df_results)\n",
    "                merged = pd.merge(a, b)\n",
    "\n",
    "                if len(merged) == 0:\n",
    "                    print(f'No results for {result_file}')\n",
    "                    continue\n",
    "\n",
    "                highs = merged[merged['literal'] > 0.5]\n",
    "                true_high = (highs[f'score {category_a}'] > highs[f'score {category_b}']).sum()\n",
    "                                    \n",
    "                lows = merged[merged['literal'] <= 0.5]\n",
    "                true_low = (lows[f'score {category_a}'] < lows[f'score {category_b}']).sum()\n",
    "                \n",
    "                acc = (true_high + true_low) / len(merged)\n",
    "                acc_scores.append(acc)\n",
    "\n",
    "                # MR diff\n",
    "                # filter dataframes to only contain the relevant entities\n",
    "                no_human_results = df_results[~df_results['head'].isin(human_uris.to_list())]\n",
    "                human_results = df_results[df_results['head'].isin(human_uris.to_list())]\n",
    "\n",
    "                # compute MR for human and non-human entities\n",
    "                mr_no_human = no_human_results[no_human_results['relation']==value_relation]['rank tail'].mean()\n",
    "                mr_human = human_results[human_results['relation']==value_relation]['rank tail'].mean()\n",
    "\n",
    "                mr_diff = mr_no_human - mr_human\n",
    "                mr_diff_scores.append(mr_diff)\n",
    "\n",
    "        acc_scores_mean, acc_scores_std = round(pd.Series(acc_scores).mean(), 3), round(pd.Series(acc_scores).std(), 3)\n",
    "        mr_diff_scores_mean, mr_diff_scores_std = round(pd.Series(mr_diff_scores).mean(), 3), round(pd.Series(mr_diff_scores).std(), 3)\n",
    "\n",
    "        line_string += f' & ${acc_scores_mean:.3f} {{\\scriptstyle \\pm {acc_scores_std:.3f}}}$'\n",
    "\n",
    "    out.write(line_string + '\\\\\\\\ \\n')\n",
    "\n",
    "out.write(tail)\n",
    "out.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-19T15:18:29.470758Z",
     "start_time": "2024-06-19T15:18:28.290735Z"
    }
   },
   "outputs": [],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
